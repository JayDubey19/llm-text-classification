{
  "README.md": "# LLM-Powered Text Classification API\n\nA minimal **FastAPI-based API** for classifying text into categories such as **toxic**, **spam**, and **safe**, powered by a Hugging Face transformer model.  \n\nThis project was built as part of an assignment to demonstrate:\n- LLM-powered content moderation\n- Prompt engineering\n- Feedback loop for continuous improvement\n- API metrics & evaluation harness\n\n---\n\n## 🚀 Features\n- **POST /classify** → Classify input text (`toxic`, `spam`, `safe`)\n- **POST /feedback** → Store feedback for retraining/evaluation\n- **GET /metrics** → Track total requests, class distribution, feedback, latency\n- **GET /healthz** → Health check\n- **Prompt Engineering** → Includes baseline and improved prompts\n- **Evaluation Harness** → Accuracy, Precision, Recall, and F1 on small dataset\n\n---\n\n## 📂 Project Structure\n```\n.\n├─ app/\n│  ├─ main.py               # FastAPI entrypoint\n│  ├─ routes/               # API route handlers\n│  ├─ services/             # Hugging Face integration\n│  ├─ prompts/              # Baseline & improved prompts\n│  └─ telemetry/            # Metrics collection\n├─ eval/\n│  ├─ dataset.jsonl         # Small evaluation dataset\n│  └─ run.py                # Evaluation script\n├─ requirements.txt         # Dependencies\n└─ README.md                # Documentation\n```\n\n---\n\n## ⚙️ Setup Instructions\n\n### 1. Clone the repo\n```bash\ngit clone https://github.com/YOUR-USERNAME/llm-text-classification.git\ncd llm-text-classification\n```\n\n### 2. Create and activate virtual environment\n```bash\npython -m venv venv\n# Windows\nvenv\\Scripts\\activate\n# macOS/Linux\nsource venv/bin/activate\n```\n\n### 3. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n### 4. Run the API\n```bash\nuvicorn app.main:app --reload\n```\n\nVisit 👉 [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) to test endpoints in Swagger UI.  \n\n---\n\n## 🛠️ API Usage\n\n### 🔹 Health Check\n```bash\nGET /healthz\n```\nResponse:\n```json\n{\"status\": \"ok\"}\n```\n\n### 🔹 Classify Text\n```bash\nPOST /classify\n{\n  \"text\": \"I hate you\"\n}\n```\nResponse:\n```json\n{\n  \"class\": \"toxic\",\n  \"confidence\": 0.95,\n  \"prompt_used\": \"You are a helpful moderation assistant...\\nText: I hate you\\nAnswer:\",\n  \"latency_ms\": 120\n}\n```\n\n### 🔹 Send Feedback\n```bash\nPOST /feedback\n{\n  \"text\": \"I hate you\",\n  \"predicted\": \"toxic\",\n  \"correct\": \"toxic\"\n}\n```\n\n### 🔹 Metrics\n```bash\nGET /metrics\n```\nResponse example:\n```json\n{\n  \"total_requests\": 3,\n  \"class_distribution\": {\"toxic\": 2, \"safe\": 1},\n  \"feedback_counts\": {\"positive\": 1, \"negative\": 0},\n  \"latency\": {\"avg_ms\": 150, \"p95_ms\": 200}\n}\n```\n\n---\n\n## 🧠 Prompt Engineering\n\n- **Baseline Prompt**\n```text\nClassify the following text into toxic, spam, or safe:\n{text}\n```\n\n- **Improved Prompt**\n```text\nYou are a helpful moderation assistant.\nAnalyze the user text carefully and classify it as one of:\n- toxic (offensive or hateful)\n- spam (irrelevant or promotional)\n- safe (harmless)\n\nText: {text}\nAnswer:\n```\n\n**Choice Rationale:**  \n- Baseline → zero-shot, minimal instruction.  \n- Improved → role-based, structured categories, improves accuracy.  \n\n---\n\n## 📊 Evaluation\n\nDataset: [`eval/dataset.jsonl`](eval/dataset.jsonl) (~20–30 labeled examples).  \n\nRun evaluation:\n```bash\npython -m eval.run\n```\n\nMetrics reported:\n- Accuracy\n- Precision\n- Recall\n- F1-score\n\n---\n\n## ⚖️ Design Trade-offs & Limitations\n- **In-memory feedback** → resets on server restart (could be persisted in SQLite/JSON for production).\n- **Small dataset** → only for demonstration, not robust training.\n- **CPU inference only** → slower on large models, but works for assignment/demo.\n- **Limited categories** → only `toxic`, `spam`, `safe`; can be expanded.\n\n---\n\n## 🌐 (Optional) Deployment\nThis API can be deployed to **Render**, **Railway**, or **Fly.io**.  \nStart Command:\n```bash\nuvicorn app.main:app --host 0.0.0.0 --port 10000\n```\n\n---\n\n## 🧪 Running Tests\n\nThis project includes basic tests for all endpoints using **pytest**.\n\n### Run Tests\nFrom the project root:\n```bash\npytest\n```\n\nOn Windows PowerShell, if you face `ModuleNotFoundError`, set `PYTHONPATH` explicitly:\n```powershell\n$env:PYTHONPATH=\".\"; pytest\n```\n\n### Example Output\n```\ncollected 4 items\n\ntests/test_api.py ....                                                       [100%]\n\n4 passed in 22.6s\n```\n\n✅ This confirms that `/healthz`, `/classify`, `/feedback`, and `/metrics` are all working as expected.\n\n---\n\n## ✅ Deliverables\n- [x] FastAPI app with required endpoints  \n- [x] Prompt library with baseline & improved prompts  \n- [x] Feedback persistence  \n- [x] Metrics endpoint  \n- [x] Evaluation harness with dataset  \n- [x] README with setup, usage, prompts, and evaluation  \n- [x] Basic tests for endpoints  \n\n---\n"
}
