{
  "README.md": "# LLM-Powered Text Classification API\n\nA minimal **FastAPI-based API** for classifying text into categories such as **toxic**, **spam**, and **safe**, powered by a Hugging Face transformer model.  \n\nThis project was built as part of an assignment to demonstrate:\n- LLM-powered content moderation\n- Prompt engineering\n- Feedback loop for continuous improvement\n- API metrics & evaluation harness\n\n---\n\n## ğŸš€ Features\n- **POST /classify** â†’ Classify input text (`toxic`, `spam`, `safe`)\n- **POST /feedback** â†’ Store feedback for retraining/evaluation\n- **GET /metrics** â†’ Track total requests, class distribution, feedback, latency\n- **GET /healthz** â†’ Health check\n- **Prompt Engineering** â†’ Includes baseline and improved prompts\n- **Evaluation Harness** â†’ Accuracy, Precision, Recall, and F1 on small dataset\n\n---\n\n## ğŸ“‚ Project Structure\n```\n.\nâ”œâ”€ app/\nâ”‚  â”œâ”€ main.py               # FastAPI entrypoint\nâ”‚  â”œâ”€ routes/               # API route handlers\nâ”‚  â”œâ”€ services/             # Hugging Face integration\nâ”‚  â”œâ”€ prompts/              # Baseline & improved prompts\nâ”‚  â””â”€ telemetry/            # Metrics collection\nâ”œâ”€ eval/\nâ”‚  â”œâ”€ dataset.jsonl         # Small evaluation dataset\nâ”‚  â””â”€ run.py                # Evaluation script\nâ”œâ”€ requirements.txt         # Dependencies\nâ””â”€ README.md                # Documentation\n```\n\n---\n\n## âš™ï¸ Setup Instructions\n\n### 1. Clone the repo\n```bash\ngit clone https://github.com/YOUR-USERNAME/llm-text-classification.git\ncd llm-text-classification\n```\n\n### 2. Create and activate virtual environment\n```bash\npython -m venv venv\n# Windows\nvenv\\Scripts\\activate\n# macOS/Linux\nsource venv/bin/activate\n```\n\n### 3. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n### 4. Run the API\n```bash\nuvicorn app.main:app --reload\n```\n\nVisit ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) to test endpoints in Swagger UI.  \n\n---\n\n## ğŸ› ï¸ API Usage\n\n### ğŸ”¹ Health Check\n```bash\nGET /healthz\n```\nResponse:\n```json\n{\"status\": \"ok\"}\n```\n\n### ğŸ”¹ Classify Text\n```bash\nPOST /classify\n{\n  \"text\": \"I hate you\"\n}\n```\nResponse:\n```json\n{\n  \"class\": \"toxic\",\n  \"confidence\": 0.95,\n  \"prompt_used\": \"You are a helpful moderation assistant...\\nText: I hate you\\nAnswer:\",\n  \"latency_ms\": 120\n}\n```\n\n### ğŸ”¹ Send Feedback\n```bash\nPOST /feedback\n{\n  \"text\": \"I hate you\",\n  \"predicted\": \"toxic\",\n  \"correct\": \"toxic\"\n}\n```\n\n### ğŸ”¹ Metrics\n```bash\nGET /metrics\n```\nResponse example:\n```json\n{\n  \"total_requests\": 3,\n  \"class_distribution\": {\"toxic\": 2, \"safe\": 1},\n  \"feedback_counts\": {\"positive\": 1, \"negative\": 0},\n  \"latency\": {\"avg_ms\": 150, \"p95_ms\": 200}\n}\n```\n\n---\n\n## ğŸ§  Prompt Engineering\n\n- **Baseline Prompt**\n```text\nClassify the following text into toxic, spam, or safe:\n{text}\n```\n\n- **Improved Prompt**\n```text\nYou are a helpful moderation assistant.\nAnalyze the user text carefully and classify it as one of:\n- toxic (offensive or hateful)\n- spam (irrelevant or promotional)\n- safe (harmless)\n\nText: {text}\nAnswer:\n```\n\n**Choice Rationale:**  \n- Baseline â†’ zero-shot, minimal instruction.  \n- Improved â†’ role-based, structured categories, improves accuracy.  \n\n---\n\n## ğŸ“Š Evaluation\n\nDataset: [`eval/dataset.jsonl`](eval/dataset.jsonl) (~20â€“30 labeled examples).  \n\nRun evaluation:\n```bash\npython -m eval.run\n```\n\nMetrics reported:\n- Accuracy\n- Precision\n- Recall\n- F1-score\n\n---\n\n## âš–ï¸ Design Trade-offs & Limitations\n- **In-memory feedback** â†’ resets on server restart (could be persisted in SQLite/JSON for production).\n- **Small dataset** â†’ only for demonstration, not robust training.\n- **CPU inference only** â†’ slower on large models, but works for assignment/demo.\n- **Limited categories** â†’ only `toxic`, `spam`, `safe`; can be expanded.\n\n---\n\n## ğŸŒ (Optional) Deployment\nThis API can be deployed to **Render**, **Railway**, or **Fly.io**.  \nStart Command:\n```bash\nuvicorn app.main:app --host 0.0.0.0 --port 10000\n```\n\n---\n\n## ğŸ§ª Running Tests\n\nThis project includes basic tests for all endpoints using **pytest**.\n\n### Run Tests\nFrom the project root:\n```bash\npytest\n```\n\nOn Windows PowerShell, if you face `ModuleNotFoundError`, set `PYTHONPATH` explicitly:\n```powershell\n$env:PYTHONPATH=\".\"; pytest\n```\n\n### Example Output\n```\ncollected 4 items\n\ntests/test_api.py ....                                                       [100%]\n\n4 passed in 22.6s\n```\n\nâœ… This confirms that `/healthz`, `/classify`, `/feedback`, and `/metrics` are all working as expected.\n\n---\n\n## âœ… Deliverables\n- [x] FastAPI app with required endpoints  \n- [x] Prompt library with baseline & improved prompts  \n- [x] Feedback persistence  \n- [x] Metrics endpoint  \n- [x] Evaluation harness with dataset  \n- [x] README with setup, usage, prompts, and evaluation  \n- [x] Basic tests for endpoints  \n\n---\n"
}
